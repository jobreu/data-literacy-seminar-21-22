{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Tweets explorieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesem Notebook können Sie Ihre eigenen Tweets explorieren. Um dieses mit ihren eigenen Daten nutzen zu können, müssen die zuvor Ihr [Twitter-Archiv herunterladen](https://help.twitter.com/en/managing-your-account/how-to-download-your-twitter-archive). Nachdem Sie Ihr Archiv gespeichert haben, müssen Sie die `.zip`-Datei zunächst entpacken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis**: Ihr Twitter-Archiv enthält mehr Daten als nur Ihre Tweets. Wenn Sie die Datei entpackt haben, können Sie Ihre Daten auch über ihren Browser explorieren. Hierzu müssen Sie auf Ihrem Rechner lokal die Datei `Your archive.html` öffnen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten liegen in Ihrem Twitter-Archiv-Ordner im Unterordner `data` im [JSON-Format](https://de.wikipedia.org/wiki/JavaScript_Object_Notation) vor. JSON-Dateien haben eigentlich die Endung `.json`. Twitter nutzt für die Exportdateien jedoch eine nicht-standardisierte Version von JSON-Listen mit der Dateiendung `.js` (die eigentlich für JavaScript-Dateien genutzt wird). Um die Daten in `R` einlesen zu können, müssen die Dateien zuerst manuell verändert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für dieses Notebook nutzen wir die Daten zu den Tweets aus der Datei `tweet.js`. Um diese mit `R` einlesen und explorieren zu können, müssen Sie zwei Dinge (lokal auf Ihrem Rechner) machen:\n",
    "1. Die Datei `tweet.js` mit einem Texteditor wie [Notepad++](https://notepad-plus-plus.org/) oder [Atom](https://atom.io/) öffnen und den Text vor der ersten eckigen Klammer löschen (in meiner Datei war dies der folgende Text: \"window.YTD.tweet.part0 = \").\n",
    "1. Die Datei mit der Endung `.json` speichern (z.B. als `tweet.json`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem Sie dies gemacht haben, müssen Sie die Datei hier noch hochladen, um damit arbeiten zu können. Die Datei sollte im Ordner *data* gespeichert werden. Öffnen Sie diesen (durch Doppelklick auf den Ordnernamen) im File Explorer auf der linken Seite und nutzen dann den *Upload Files*-Button im Menü oben links (das Symbol ist ein Pfeil über einem Strich). Wählen Sie darüber die JSON-Datei, die Sie gerade erstellt haben, von Ihrem lokalen Rechner aus und laden Sie diese in den Ordner *data* hoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis**: Wenn Sie Twitter schon sehr lange nutzen und häufig tweeten, wird die Datei, die Ihre Tweets enthält, recht groß sein. In diesem Fall können der Upload sowie das Einlesen (s.u.) etwas länger dauern (dies gilt ggf. auch für das lokale Öffnen und Bearbeiten der Datei)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zur Erinnerung**: Ihre persönliche Kopie des Notebooks sowie alle Dateien, die Sie hochladen, werden am Ende der Nutzungssitzung gelöscht. Wenn Sie aber ganz \"auf Nummer sicher gehen\" wollen, können Sie die Datei mit Ihren Tweets auch über den File Explorer auf der linken Seite nach dem Durcharbeiten dieses Notebooks auch manuell löschen: Rechtsklick auf den Dateinamen und dann *Delete* auswählen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pakete laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie auch in den anderen Notebooks müssen wir zunächst die benötigten `R`-Pakete laden. Das sind diesmal ein paar mehr, da wir mit Textdaten und Zeitstempeln arbeiten und diese für unsere Analysen und Visualisierungen entsprechend aufbereiten müssen. Bevor wir die Pakete laden können, müssen wir noch das Paket `stopwords` (nach-)installieren, um später im Notebook auch mit deutschsprachigen Tweets arbeiten zu können (NB: die Installation dieses Pakets kann ein paar Minuten dauern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(jsonlite)\n",
    "library(tidyverse)\n",
    "library(lubridate)\n",
    "library(scales)\n",
    "library(tidytext)\n",
    "library(stopwords)\n",
    "library(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt lesen wir die Daten ein und wandeln sie in ein `data.frame`-Objekt um, mit dem wir in `R` arbeiten können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_json <- fromJSON(\"./data/tweet.json\") \n",
    "tweets_df <- jsonlite::flatten(tweets_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen ersten Eindruck von den Daten zu bekommen, können wir uns die Namen der enthaltenden Variablen sowie die ersten 10 Zeilen im Datensatz anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(tweets_df)\n",
    "head(tweets_df, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweis: In der Dokumentation seiner API bietet Twitter u.a. auch eine [Beschreibung der Datenstruktur von JSON-Files für Tweets an](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet), aus der ersichtlich wird, welche Daten diese enthalten. Diese Beschreibung ist im Prinzip das Analogon zu einem Codebuch für Befragungsdaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten aufbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Daten auswerten und visualisieren zu können, müssen sie in ein entsprechendes Format gebracht werden. Hierzu passen wir als ersten Schritt die Zeitstempel für die Tweets an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df <- tweets_df %>% \n",
    "  mutate(timestamp = parse_date_time(tweet.created_at, \n",
    "                                     orders = \"abdHMSzY\")) %>% \n",
    "  mutate(timestamp = with_tz(timestamp, tzone = \"Europe/Berlin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wann haben Sie Ihren ersten Tweet geposted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(tweets_df$timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzahl von Tweets über die Zeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun visualisieren wir die Anzahl von Tweets pro Monat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df %>%\n",
    "  mutate(time_floor = floor_date(timestamp, unit = \"1 month\")) %>%\n",
    "  count(time_floor) %>%\n",
    "  ggplot(aes(x = as.Date(time_floor), y = n)) +\n",
    "  geom_bar(stat = \"identity\") +\n",
    "  scale_x_date(labels = date_format(\"%m-%Y\"),\n",
    "               date_breaks = \"1 year\",\n",
    "               date_minor_breaks = \"1 month\",\n",
    "               expand = expansion(mult=c(0,0))) +\n",
    "  scale_y_continuous(expand = expansion(mult=c(0,0.1))) +\n",
    "  labs(title = \"Anzahl von Tweets pro Monat\",\n",
    "       x = \"Monat\",\n",
    "       y = \"Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wann tweeten Sie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An welchen Wochentagen tweeten Sie besonders häufig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df %>% \n",
    "  ggplot(aes(x = wday(timestamp, label = TRUE, week_start = 1), y = stat(count))) +\n",
    "  geom_bar() +\n",
    "  theme(legend.position = \"none\") +\n",
    "  xlab(\"Day of the Week\") + ylab(\"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zu welchen Uhrzeiten tweeten Sie am meisten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df %>% \n",
    "  ggplot(aes(x = hour(timestamp), y = stat(count))) +\n",
    "  geom_bar() +\n",
    "  theme(legend.position = \"none\") +\n",
    "  xlab(\"Hour of the Day\") + ylab(\"Number of tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beliebteste Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Sache, die wir uns anschauen können, ist, welche Tweets am populärsten waren. Hierzu nutzen wir den Favorite Count. Alternativ könnten wir auch den Retweet Count nutzen (die entsprechende Variable heißt `tweet.retweet_count`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df %>% \n",
    "  arrange(desc(tweet.favorite_count)) %>% \n",
    "  select(tweet.id, tweet.full_text, tweet.favorite_count) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replies, Hashtags und Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie viel Prozent Ihrer Tweets sind Replies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(sum(!is.na(tweets_df$tweet.in_reply_to_status_id))/nrow(tweets_df)*100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcher Anteil Ihrer Tweets sind Retweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df %>% \n",
    "  mutate(is_retweet = ifelse(str_detect(tweet.full_text, \"^RT\"), 1, 0)) %>% \n",
    "  count(is_retweet) %>% \n",
    "  mutate(percentage = round(n/nrow(tweets_df)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie viele Ihrer Tweets enthalten Hashtags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df %>% \n",
    "  mutate(has_hashtag = ifelse(str_detect(tweet.full_text, \"#\"), 1, 0)) %>% \n",
    "  count(has_hashtag) %>% \n",
    "  mutate(percentage = round(n/nrow(tweets_df)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worthäufigkeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu explorieren, welche Wörter Sie am häufigsten nutzen, müssen wir die Daten noch weiter aufbereiten. Hierzu müssen wir Verfahren aus dem sogenannten [Text Mining](https://de.wikipedia.org/wiki/Text_Mining) einsetzen. Eine Einführung in das Text Mining ist im Rahmen dieses Notebooks (sowie auch dieses Seminars) nicht möglich. Im Netz gibt es jedoch zahlreiche gute Einführungen, wie z.B.:\n",
    "- [Text Mining with R - A Tidy Approach](https://www.tidytextmining.com/) von Julia Silge und David Robinson\n",
    "- [Text mining in R for the social sciences and digital humanities](https://tm4ss.github.io/docs/) von Andreas Niekler und Gregor Wiedemann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine sehr ausführliche Einführung in die Arbeit mit Textdaten bietet auch die Webseite [Automatisierte Inhaltsanalyse mit R](http://inhaltsanalyse-mit-r.de/index.html) von Cornelius Puschmann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nutzen im Folgenden das Paket [`tidytext`](https://github.com/juliasilge/tidytext) und der Code basiert in weiten Teilen auf den Beispielen aus Kapitel 7 des Buchs *Tidy Text Mining with R*. Ein anderes, ebenfalls sehr umfangreiches Paket für die Analyse von Textdaten in `R` ist [`quanteda`](https://quanteda.io/), zu dem es im Netz auch sehr umfangreiche [Tutorials](https://tutorials.quanteda.io/) gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Aufbereitungsschritt teilen wir die Tweets in einzelne Wörter auf. Dies nennt man [Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization). Zudem entfernen wir einige bestimmte Zeichenfolgen sowie sogenannte [Stoppwörter](https://de.wikipedia.org/wiki/Stoppwort) (d.h., Wörter, die sehr häufig in Texten vorkommen, für deren Inhalt jedoch i.d.R. nicht besonders wichtig sind) und schließen Retweets aus. Hierfür nutzen wir sogenannte [Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ich selbst tweete auf Deutsch sowie auf Englisch. Da Stoppwörter sprachspezifisch sind, beschränken wir die Auswahl der Tweets im nachfolgenden Code auf solche, die Twitter als englischsprachig erkannt hat (Hinweis: Dies funktioniert natürlich nicht 100%ig; z.B. bei Tweets, die in mehreren Sprachen verfasst sind)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_reg <- \"&amp;|&lt;|&gt;\"\n",
    "\n",
    "tidy_tweets <- tweets_df %>% \n",
    "  rename(text = tweet.full_text) %>% \n",
    "  filter(!str_detect(text, \"^RT\"),\n",
    "         tweet.lang == \"en\") %>%\n",
    "  mutate(text = str_remove_all(text, remove_reg)) %>%\n",
    "  unnest_tokens(word, text, token = \"tweets\") %>%\n",
    "  filter(!word %in% stop_words$word,\n",
    "         !word %in% str_remove_all(stop_words$word, \"'\"),\n",
    "         str_detect(word, \"[a-z]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir Worthäufigkeiten zählen. Hierbei schließen wir @-mentions aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs <- tidy_tweets %>% \n",
    "  filter(!str_detect(word, \"^@\")) %>% \n",
    "  count(word) %>% \n",
    "  rename(freq = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was sind die häufigsten Wörter in Ihren englischsprachigen Tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_tweets %>% \n",
    "  filter(!str_detect(word, \"^@\")) %>% \n",
    "  count(word, sort = T) %>% \n",
    "  top_n(10) %>% \n",
    "  mutate(word = reorder(word, n)) %>% \n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col() +\n",
    "  labs(x = NULL, y = \"Häufigkeit\") +\n",
    "  coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selbstverständlich können wir dasselbe auch für deutschsprachige Tweets machen. Hierfür müssen wir zusätzlich die entsprechende Liste mit Stoppwörtern laden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was sind die häufigsten Wörter in Ihren deutschsprachigen Tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_de <- get_stopwords(language = \"de\")\n",
    "\n",
    "saubere_zwitscher <- tweets_df %>% \n",
    "  rename(text = tweet.full_text) %>% \n",
    "  filter(!str_detect(text, \"^RT\"),\n",
    "         tweet.lang == \"de\") %>%\n",
    "  mutate(text = str_remove_all(text, remove_reg)) %>%\n",
    "  unnest_tokens(word, text, token = \"tweets\") %>%\n",
    "  filter(!word %in% stop_words_de$word,\n",
    "         str_detect(word, \"[a-z]\"))\n",
    "\n",
    "wrt_hfgkt <- saubere_zwitscher %>% \n",
    "  filter(!str_detect(word, \"^@\")) %>% \n",
    "  count(word) %>% \n",
    "  rename(freq = n)\n",
    "\n",
    "wrt_hfgkt %>% \n",
    "  arrange(-freq) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich der häufigsten deutschen und englischen Wörter in Ihren Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend kombinieren wir die englischsprachigen und deutschsprachigen Tweets und vergleichen visuell die jeweiligen Worthäufigkeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en <- tidy_tweets %>% \n",
    "  select(word, tweet.lang) %>% \n",
    "  rename(language = tweet.lang)\n",
    "\n",
    "tweets_de <- saubere_zwitscher %>% \n",
    "  select(word, tweet.lang) %>% \n",
    "  rename(language = tweet.lang)\n",
    "\n",
    "tweets_combined <- tweets_en %>% \n",
    "  bind_rows(tweets_de)\n",
    "\n",
    "tweets_combined %>% \n",
    "  filter(!str_detect(word, \"^@\"),\n",
    "         !str_detect(word, \"^#\")) %>% \n",
    "  count(word, language, sort = T) %>% \n",
    "  ungroup %>% \n",
    "  group_by(language) %>%\n",
    "  top_n(10, n) %>%\n",
    "  ungroup() %>%\n",
    "  mutate(word = reorder(word, n, fill = language)) %>% \n",
    "  ggplot(aes(x = word, y = n)) +\n",
    "  geom_col(show.legend = FALSE) +\n",
    "  facet_wrap(~language, scales = \"free_y\") +\n",
    "  labs(x = NULL, y = \"Häufigkeit\") +\n",
    "  coord_flip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Häufigste Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welche Hashtags verwenden Sie am häufigsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_combined %>% \n",
    "  filter(str_detect(word, \"^#\")) %>% \n",
    "  count(word) %>%\n",
    "  arrange(-n) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wortwolken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine populäre Form der Visualisierung von Textdaten sind Wortwolken (word clouds). Auch diese lassen sich mit `R` einfach erstellen (*NB*: wie so oft in `R`, gibt es auch hierfür verschiedene Pakete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud(words = word_freqs$word,\n",
    "          freq = word_freqs$freq,\n",
    "          min.freq = 10,\n",
    "          max.words = 50,\n",
    "          random.order = FALSE,\n",
    "          rot.per = 0.35,\n",
    "          colors = brewer.pal(8, \"Dark2\"))\n",
    "\n",
    "wordcloud(words = wrt_hfgkt$word,\n",
    "          freq = wrt_hfgkt$freq,\n",
    "          min.freq = 5,\n",
    "          max.words = 50,\n",
    "          random.order = FALSE,\n",
    "          rot.per = 0.35,\n",
    "          colors = brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch umfangreichere Daten über die eigenen Twitter-Aktivitäten sowie die anderer Nutzer\\_innen kann man über die [Twitter API](https://developer.twitter.com/en/docs/twitter-api) sammeln. In `R` geht dies am einfachsten und umfassendsten mit dem Paket [`rtweet`](https://docs.ropensci.org/rtweet/). Zu diesem gibt es im Netz verschiedene Tutorials, wie z.B. die [*21 Recipes for Mining Twitter Data with rtweet*](https://rud.is/books/21-recipes/) von Bob Rudis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seit einiger Zeit gibt es eine [neue Version der Twitter API (v2)](https://developer.twitter.com/en/docs/twitter-api). Für diese gibt es auch eine [eigene Zugangsoption für die akademische Forschung](https://developer.twitter.com/en/products/twitter-api/academic-research), die noch umfangreicheren Datenzugriff erlaubt als die kostenfreie Version der Standard-API v1.1. Auch für diese Zugriffsoption gibt es diverse R-Pakete. Das umfangreichste ist [academictwitteR](https://github.com/cjbarrie/academictwitteR)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
